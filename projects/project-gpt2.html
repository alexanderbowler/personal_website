<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>GPT-2 124M Model | Projects</title>
  <link rel="stylesheet" href="../styles.css" />
</head>
<body>
  <header>
    <div class="wrap">
      <h1>Project: GPT-2 124M Model</h1>
      <div class="section-note">Recreating a foundational language model with modern training tooling.</div>
      <nav>
        <a href="../index.html">About</a>
        <span>/</span>
        <a href="../resume.html">Resume</a>
        <span>/</span>
        <a href="index.html">Projects</a>
        <span>/</span>
        <a href="project-gpt2.html">Project: GPT-2 124M</a>
        <span>/</span>
        <a href="project-traffic.html">Project: Traffic Sign Classifier</a>
        <span>/</span>
        <a href="project-backprop.html">Project: Interpreter</a>
      </nav>
    </div>
  </header>

  <main>
    <div class="figure">
      <img src="../transformer_architecture.png" alt="Transformer architecture diagram for the GPT-2 model." />
    </div>

    <section>
      <h2>Overview</h2>
      <p>
        This project recreates OpenAI’s GPT-2 124M architecture based on the GPT-2 and GPT-3 papers
        and Andrej Karpathy’s GPT series. The emphasis was on faithful architecture replication and
        robust multi-GPU training.
      </p>
    </section>

    <section>
      <h2>Problem</h2>
      <p>
        Reproducing a large language model is challenging due to scale, stability, and data throughput.
        The goal was to implement the model faithfully while building training tooling that could run
        efficiently on multiple GPUs.
      </p>
    </section>

    <section>
      <h2>Approach</h2>
      <ul class="list-tight">
        <li>Implemented GPT-2 blocks and attention layers with clear weight initialization.</li>
        <li>Wrote a multi-GPU training script to leverage 8x H100s effectively.</li>
        <li>Tracked loss curves and checkpointed to validate training stability.</li>
      </ul>
    </section>

    <section>
      <h2>Outcome</h2>
      <p>
        The final run achieved lower loss than the original GPT-2 124M baseline. The repo serves as a
        reference implementation and experimentation bed for further scaling.
      </p>
    </section>

    <section>
      <h2>Tools</h2>
      <p>Python, PyTorch, HuggingFace tooling, distributed training on H100 GPUs.</p>
    </section>

    <section>
      <h2>Links</h2>
      <p><a href="https://github.com/alexanderbowler/GPT-2">GitHub Repository</a></p>
    </section>

    <section>
      <h2>Related</h2>
      <p>Listed in the <a href="../resume.html">resume</a> under selected projects.</p>
    </section>
  </main>

  <footer>
    Next project: <a href="project-traffic.html">Traffic Sign Classifier</a>
  </footer>
</body>
</html>
